---
title: "main"
output:
  pdf_document: default
  html_document: default
date: "2023-02-10"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, # show code
                      results = T, # show output text
                      #fig.show='hide', # show plot
                      error = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      tidy = TRUE, # pretty code
                      comment = "",
                      attr.source = ".numberLines")
```


```{r message=FALSE, warning=FALSE, echo=FALSE}
#if not installed install all the following libraries
library(tseries)
library(visdat)
library(raster)
library(ggplot2)
library(gridExtra)
library(forecast)
library(svglite)
```
# Data cleaning

This section describes the procedure used to clean the data relating to the Emilia Romagna and Lombardy regions. This data will be used by the mcmc algorithm.

## Emilia Romagna

### Quick way

The whole procedure can be done quickly with the following lines of code. (This way one can go directly to the next section "Lombardy"):

```{r}
source("utils/read_ts_from_emilia.R")

# choose a year from 2014 to 2019
anno = "2018"

n_giorni = 365
if(anno=="2016"){n_giorni = 366}

ob1 = read_ts_from_emilia(anno = anno)
localita_e = ob1$localita
area_localita_e = ob1$area_localita
tipo_localita_e = ob1$tipo_localita
dati_completi_set_e = t(ob1$time_series_sett)
```

### Datailed procedure

```{r}
# choose a year from 2014 to 2019
anno = "2018"
n_giorni = 365
if(anno=="2016"){n_giorni = 366}
PM10_Emilia = read.csv("input_data/PM10_Emilia.csv")
```

```{r}
# missing data:
#sum(is.na.data.frame(PM10_Emilia))
library(visdat)
vis_dat(PM10_Emilia, warn_large_data = FALSE)
```

```{r}
# remove rows without location
head(PM10_Emilia[which(is.na(PM10_Emilia[,"NomeStazione"])),])
PM10_Emilia = na.omit(PM10_Emilia)
```

```{r}
# split data based on locations
localita_e = unique(PM10_Emilia[,"NomeStazione"])
p_e = length(localita_e)

data_e = PM10_Emilia[which(PM10_Emilia[,"Anno"]== anno),]


data_split_e = vector(mode = "list", length = p_e)
for(i in 1:p_e)
{
  data_split_e[[i]] = data_e[which(data_e[,"NomeStazione"]==localita_e[i]), ]
}
```

```{r}
# in many locations there are not 365/366 observations
numero_dati_e = nrow(data_split_e[[1]])
for(i in 2:p_e)
  numero_dati_e = c(numero_dati_e, nrow(data_split_e[[i]]))
plot(numero_dati_e,pch=19,xlab = "Locations", ylab="Number of data in a year", ylim = c(0,370))
abline(h=n_giorni,col = "red", lty = 2, lwd = 3)
```

```{r}
# insert NA where there are hidden missing data
# import info about years
dati_anni = read.delim2("input_data/dati_anni.txt", header=FALSE)
dati_anni = dati_anni[-c(2,5,6)]
anno_vett = c(rep("2014",365),rep("2015",365),rep("2016",366),rep("2017",365),rep("2018",365),rep("2019",365))
dati_anni = cbind(dati_anni,anno_vett)
colnames(dati_anni)=c("numero", "giorno","settimana","anno")
dati_anno = dati_anni[which(dati_anni$anno==anno),]
```

```{r}
# build a matrix 
dati_completi_e = matrix(NA, nrow = n_giorni, ncol = p_e)
colnames(dati_completi_e) = localita_e

variabile = "Valore"

for(i in 1:p_e) # scorre le colonne della matrice
{
  k = 1
  n = nrow(data_split_e[[i]])
  for(j in 1:n_giorni)
  {
    if(data_split_e[[i]][k,"Wday"] == dati_anno[j,"giorno"] & k<=n)
    {
      dati_completi_e[j,i] = data_split_e[[i]][k,variabile]
      k=k+1
    }
  }
}
```

```{r}
# view NA's places
vis_miss(data.frame(dati_completi_e))
```

```{r}
# mean by weeks
dati_completi_set_e = matrix(NA, nrow = 52, ncol = p_e)
colnames(dati_completi_set_e) = localita_e

for(i in 1:p_e)
{
  for(j in 1:52)
  {
    dati = dati_completi_e[which(dati_anno$settimana == j),i]
    dati_completi_set_e[j,i] = mean(dati,na.rm = TRUE)
    if(is.nan(dati_completi_set_e[j,i])){dati_completi_set_e[j,i]=NA}

  }
}

vis_miss(data.frame(dati_completi_set_e))
```

```{r}
# deal with na: remove columns with more than 10% of NA. If there are few NA impute missing data using interpolation.
source("utils/ts_imputation.R")
index = 0
for(i in 1:p_e)
{
  ts = dati_completi_set_e[,i]
  n_na = sum(is.na(ts))
  if(n_na > 0.05*52){
    index = c(index,i)
  } else if(n_na>0){
    dati_completi_set_e[,i] = ts_imputation(ts)
  }
}
index = index[-1]
indici = 1:p_e
indici = indici[-index]
dati_completi_set_e = dati_completi_set_e[,indici]
p_e = ncol(dati_completi_set_e)
localita_e = localita_e[indici]
```

```{r}
vis_miss(data.frame(dati_completi_set_e))
```

```{r}
# labels related to locations
area_localita_e = rep("",length(indici))
tipo_localita_e = rep("",length(indici))
j=1
for (i in indici)
{
  area_localita_e[j] = unique(data_split_e[[i]][,"Area"])
  tipo_localita_e[j] = unique(data_split_e[[i]][,"Tipo"])
  j=j+1
}
```

## Lombardy

The Lombardy data are already clean, they are only prepared for the mcmc algorithm.

```{r}
source("utils/read_ts_from_lombardia.R")
ob2 =read_ts_from_lombardia()
localita_l = ob2$localita
area_localita_l = ob2$area_localita
tipo_localita_l = ob2$tipo_localita
dati_completi_set_l = ob2$time_series_sett
```


```{r}
# merging data from the two regions
ts_e = dati_completi_set_e
ts_l = dati_completi_set_l
ts = cbind(ts_e, ts_l)
localita = c(localita_e,localita_l)
p = length(localita)
area_localita = c(area_localita_e,area_localita_l)
tipo_localita = c(tipo_localita_e,tipo_localita_l)
```


# Data plot

After running this section the plots can be found in `output_plot`

```{r}
source("utils/genera_colori.R")
source("utils/plot_time_series.R")

plot_e = plot_time_series(data = t(ts_e),
                          label1 = area_localita_e,
                          label2 = tipo_localita_e,
                          anno = paste(anno,"- Emilia Romagna"))

plot_l = plot_time_series(data = t(ts_l),
                          label1 = area_localita_l,
                          label2 = tipo_localita_l,
                          anno = "2018 - Lombardy")

plot = plot_time_series(data = t(ts),
                        label1 = area_localita,
                        label2 = tipo_localita,
                        anno = "2018 - Emilia Romagna and Lombardy")


replayPlot(plot_e$plot1)
replayPlot(plot_e$plot2)
replayPlot(plot_l$plot1)
replayPlot(plot_l$plot2)
replayPlot(plot$plot1)
replayPlot(plot$plot2)
```

```{r include=FALSE}
# saving plots
source("utils/save_svg_plot.R")
save_svg_plot(plot_e$plot1, paste("ts_area_emilia_",anno, sep=""), "output_plot/")
save_svg_plot(plot_e$plot2, paste("ts_tipo_emilia_",anno, sep=""), "output_plot/")
save_svg_plot(plot_l$plot1, paste("ts_area_lombardia_",anno, sep=""), "output_plot/")
save_svg_plot(plot_l$plot2, paste("ts_tipo_lombardia_",anno, sep=""), "output_plot/")
save_svg_plot(plot$plot1, paste("ts_area_unito_",anno, sep=""), "output_plot/")
save_svg_plot(plot$plot2, paste("ts_tipo_unito_",anno, sep=""), "output_plot/")
```



# Likelihood choice

```{r}
# remove means from the time series 
medie = colMeans(ts)
ts_no_mean = ts
for (i in 1:p) {
  ts_no_mean[,i] = ts[,i] - medie[i]
}
```


```{r}
# look at some random Acf
set.seed(1998)
n_plot = 3
plots= vector("list",n_plot*n_plot)
random_indexes = sample(1:p, n_plot*n_plot)
j = 1
for (i in random_indexes) {
  plots[[j]] = ggAcf(ts(ts_no_mean[,i]), lag.max = 52) + labs(title = localita[i])
  j = j+1
}
plot_ACF = do.call(grid.arrange, c(plots, ncol = n_plot, nrow = n_plot))
save_svg_plot(plot_ACF, name="ts_acf_random", folder="output_plot/", type="ggplot", width = 10, height = 10)
```

```{r}
# look at some random Pacf
n_plot = 3
plots= vector("list",n_plot*n_plot)
j = 1
for (i in random_indexes) {
  plots[[j]] = ggPacf(ts(ts_no_mean[,i]), lag.max = 52) + labs(title = localita[i])
  j = j+1
}
plot_PACF = do.call(grid.arrange, c(plots, ncol = n_plot, nrow = n_plot))
save_svg_plot(plot_PACF, name="ts_pacf_random", folder="output_plot/", type="ggplot", width = 6.5, height = 6.5)
```

From the previous plots it seems that an AR1 or AR3 model could be fine

```{r}
fitted_models_AR1 = vector(mode="list",length = p)
fitted_models_best = vector(mode="list",length = p)
coefficienti_AR1 = rep(0,p)
for(i in 1:p)
{
  fitted_models_AR1[[i]] = arima(ts_no_mean[,i], order = c(1,0,0), include.mean = FALSE)
  coefficienti_AR1[i] = fitted_models_AR1[[i]]$coef
  fitted_models_best[[i]] = auto.arima(ts_no_mean[,i])
}

aic_AR1 = rep(0,p)
aic_best = rep(0,p)
for(i in 1:p)
{
  aic_AR1[i] = fitted_models_AR1[[i]]$aic
  aic_best[i] = fitted_models_best[[i]]$aic
}
```

```{r}
plot(aic_AR1, pch=19, xlab = "Locations", ylab="aic",type='l', main="AIC comparison between AR(1) and the best ARIMA model")
points(aic_best, pch=19, col="red",type='l')
legend("topleft", legend = c("AR(1)", "Best ARIMA model"), fill = c("black","red"), bty="n", cex=0.8)
plot_aic = recordPlot()
```

```{r include=FALSE}
# save plot
save_svg_plot(plot_aic, name=paste("aic_comparison_",anno,sep=""), folder="output_plot/")
```


```{r}
source("utils/plot_coefficienti_AR1.R")
plot_coeff=plot_coefficienti_AR1(coefficienti_AR1, area_localita, tipo_localita)
replayPlot(plot_coeff$plot1)
replayPlot(plot_coeff$plot2)
```

```{r, include=FALSE}
# save plot
save_svg_plot(plot_coeff$plot1, name=paste("coeff_area_",anno,sep=""), folder="output_plot/")
save_svg_plot(plot_coeff$plot2, name=paste("coeff_tipo_",anno,sep=""), folder="output_plot/")
```


# Prior density's hyperparameters

Data from Emilia Romagna available in the previous year (2017) is used in order to set the hyperparameters with frequentist estimates.

```{r}
anno2 = "2017"
ob3 = read_ts_from_emilia(anno = anno2)
plot_old = plot_time_series(data = t(ob3$time_series_sett),
                            label1 = ob3$area_localita,
                            label2 = ob3$tipo_localita,
                            anno = "2017 - Emilia Romagna")
replayPlot(plot_old$plot1)
replayPlot(plot_old$plot2)
```


```{r, include=FALSE}
save_svg_plot(plot_old$plot1, name=paste("ts_area_emilia_",anno2,sep=""), folder="output_plot/")

save_svg_plot(plot_old$plot2, name=paste("ts_tipo_emilia_",anno2,sep=""), folder="output_plot/")
```



```{r}
source("utils/calcola_param_prior.R")
ob4 = calcola_param_prior(ob3$time_series_sett_no_mean)
ob4$parametri
```

```{r}
plot_coeff_old = plot_coefficienti_AR1(ob4$coefficienti, ob3$area_localita, ob3$tipo_localita)
replayPlot(plot_coeff_old$plot1)
replayPlot(plot_coeff_old$plot2)
```




# Cohesion function's hyperparameters

## Choice of M

One need to include spatial information in order to run the mcmc algorithm.


## Choice of a

Study of the coordinate

```{r}
source("utils/calcola_coordinate.R")
ob5 = calcola_coordinate(localita)
coord = ob5$coordinate_lat_long
lonlat_coord = ob5$coordinate_long_lat
```


```{r message=FALSE, warning=FALSE, echo=FALSE}
library(MASS)
library(rgl)
library(DepthProc)
library(hexbin)
library(aplpack)
library(robustbase)
library(raster)
```


Study of `a` with Raster library

```{r}
#trasformo e uso libreria raster
mat_raster <- pointDistance(lonlat_coord, lonlat=T)/ 1000
mat_raster <- lower.tri(mat_raster, diag=FALSE)*mat_raster
mat_raster <- sort(as.numeric(mat_raster[!is.na(mat_raster) & mat_raster>0]))
list(
  range = range(mat_raster),
  mean_d = mean(mat_raster),
  quantiles = quantile(mat_raster, c(0.05,0.25,0.5,0.75, 0.8, 0.95)),
  max = max(mat_raster)
)
```

Study of `a` with Haversine formula

```{r}
cost <- pi/180
n <- nrow(coord)
  dist_matrix <- matrix(NA, n, n)
  for (i in 1:(n - 1)) {
    for (j in (i + 1):n) {
      lat1 <- coord[i, 1]*cost
      lon1 <- coord[i, 2]*cost
      lat2 <- coord[j, 1]*cost
      lon2 <- coord[j, 2]*cost
      a <- sin((lat2 - lat1)/2)^2 + cos(lat1) * cos(lat2) * sin((lon2 - lon1)/2)^2
      c <- 2 * atan2(sqrt(a), sqrt(1 - a))
      d <- 6371 * c
      dist_matrix[i, j] <- d
      dist_matrix[j, i] <- d
    }
  }
#dist_matrix
  
  matd3 <- lower.tri(dist_matrix, diag=FALSE)*dist_matrix
  matd3 <- sort(as.numeric(matd3[!is.na(matd3) & matd3>0]))
list(
  range = range(matd3),
  mean_d = mean(matd3),
  quantiles = quantile(matd3, c(0.05,0.25,0.5,0.75, 0.8, 0.95)),
  max = max(matd3)
)
```

# Output saving

```{r}
folder = "bayesmix/resources/datasets/"

write.table(t(ts_no_mean),sep=",", row.names = FALSE, col.names=FALSE , file = paste(folder, "ts.csv", sep=""))

write.table(t(ts),sep=",", row.names = FALSE, col.names=FALSE , file = paste(folder, "ts_mean.csv", sep=""))

write.table(coord,sep=",", row.names = FALSE, col.names=FALSE , file = paste(folder, "coord.csv", sep=""))
```


# MCMC algorithm

Now everything is ready to run the mcmc algorithm.

Open an Ubuntu terminal and run the following in the `PM10_BAYESIAN` folder to set the desired parameter:

Number of iterations and burnin:

`nano bayesmix/examples/ar1nig_hierarchy/in/algo.asciipb`

Cohesion function's hyperparameters (done better in run.sh):

`nano bayesmix/examples/ar1nig_hierarchy/in/dp.asciipb`

Prior's hyperparameters (don't change unless you have a good reason):

`nano bayesmix/examples/ar1nig_hierarchy/in/ts.asciipb`


*To run the mcmc algorithm:*

`chmod +x run.sh`

`./run.sh`




# Results

```{r}
source("utils/plot_clustering.R")
source("utils/save_svg_plot.R")

cohesion_params = read.table('used_parameters.txt', header=F)
M <- cohesion_params[1,1]
a <- cohesion_params[2,1]

num_clust = read.csv("bayesmix/examples/ar1nig_hierarchy/out/numclust.csv", header = FALSE)
names(num_clust)="n"

partitions = read.csv("bayesmix/examples/ar1nig_hierarchy/out/clustering.csv", header = FALSE)

state_params = read.csv("bayesmix/examples/ar1nig_hierarchy/out/stateparams.csv", header = FALSE)

coord = read.csv("bayesmix/resources/datasets/coord.csv", header=F)
coord = cbind(coord, localita)
colnames(coord) = c("lat","lon","loc")

ts_read = read.csv("bayesmix/resources/datasets/ts_mean.csv", header=F)
```


## Number of clusters distribution

```{r fig.height=6, fig.width=9}
x_axis_labels <- min(num_clust$n):max(num_clust$n)


title = paste("Parameters: M = ", M, ", a = ",a," Km",sep="")
plot_nclus = ggplot(num_clust,aes(x = n)) + 
              theme_minimal() +
           geom_histogram(binwidth = 1,fill="#153b70",color="white") + 
              ggtitle(title) + 
              scale_x_continuous(labels = x_axis_labels, breaks = x_axis_labels) + 
              xlab("Number of clusters") +
              ylab("Iterations") +
              theme(axis.text=element_text(size=rel(2.5)),
                    axis.title=element_text(size=rel(2.5)),
                    plot.title = element_text(face="bold", size=rel(2.5), hjust = 0.5),
                    legend.text = element_text(size=rel(1.15)),
                    legend.position = 'none')

nome = paste("hist_n_clust_", M, "_a_",a,sep="")
save_svg_plot(plot_nclus, name=nome, folder = "output_plot/", type = "ggplot")

plot_nclus
```

## Analysis of the best partition

```{r fig.height=6, fig.width=9}
source("utils/plot_clustering.R")
ob6 = plot_clustering(partitions, coord, "VI" ,M, a, ts_read, state_params)
ob6
# to plot in the console):
# x11();ob6$plot
```

```{r,include=FALSE}
# save plots
nome = paste("map_M_", M, "_a_",a,"_label",sep="")
save_svg_plot(ob6$plot_label,name=nome, folder="output_plot/", type="ggplot",width = 15, height = 15)

nome = paste("map_M_", M, "_a_",a,sep="")
save_svg_plot(ob6$plot,name=nome, folder="output_plot/", type="ggplot",width = 7.5, height = 7.5)

nome = paste("ts_output_", M, "_a_",a,sep="")
save_svg_plot(ob6$matplot, name=nome, folder = "output_plot/")

nome = paste("post_similarity_matrix_", M, "_a_",a,sep="")
save_svg_plot(ob6$psm, name=nome, folder = "output_plot/")
```



```{r}
unique_val = data.frame(value = ob6$posterior_unique_vals,
                        cluster_id = unique(ob6$best_clus))
unique_val
```










