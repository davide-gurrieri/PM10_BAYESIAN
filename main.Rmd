---
title: "main"
output:
  pdf_document: default
  html_document: default
date: "2023-02-10"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, # mostra o meno il codice
                      results = T, # mostra il testo dell' output
                      #fig.show='hide', # mostra o meno immagini
                      error = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      tidy = TRUE, # abbellisce il codice
                      comment = "",
                      attr.source = ".numberLines")
```


```{r message=FALSE, warning=FALSE, echo=FALSE}
#if not installed install all the following libraries
library(tseries)
library(visdat)
library(raster)
library(ggplot2)
library(gridExtra)
library(forecast)
library(svglite)
```
# Data cleaning

This section describes the procedure used to clean the data relating to the Emilia Romagna and Lombardy regions. This data will be used by the mcmc algorithm.

## Emilia Romagna

### Quick way

The whole procedure can be done quickly with the following lines of code. (This way one can go directly to the next section "Lombardy"):

```{r}
source("utils/read_ts_from_emilia.R")

# choose a year from 2014 to 2019
anno = "2018"

n_giorni = 365
if(anno=="2016"){n_giorni = 366}

ob1 = read_ts_from_emilia(anno = anno)
località_e = ob1$località
area_località_e = ob1$area_località
tipo_località_e = ob1$tipo_località
dati_completi_set_e = t(ob1$time_series_sett)
```

### Datailed procedure

```{r}
# choose a year from 2014 to 2019
anno = "2018"
n_giorni = 365
if(anno=="2016"){n_giorni = 366}
PM10_Emilia = read.csv("input_data/PM10_Emilia.csv")
```

```{r}
# missing data:
#sum(is.na.data.frame(PM10_Emilia))
library(visdat)
vis_dat(PM10_Emilia, warn_large_data = FALSE)
```

```{r}
# remove rows without location
head(PM10_Emilia[which(is.na(PM10_Emilia[,"NomeStazione"])),])
PM10_Emilia = na.omit(PM10_Emilia)
```

```{r}
# split data based on locations
località_e = unique(PM10_Emilia[,"NomeStazione"])
p_e = length(località_e)

data_e = PM10_Emilia[which(PM10_Emilia[,"Anno"]== anno),]


data_split_e = vector(mode = "list", length = p_e)
for(i in 1:p_e)
{
  data_split_e[[i]] = data_e[which(data_e[,"NomeStazione"]==località_e[i]), ]
}
```

```{r}
# in many locations there are not 365/366 observations
numero_dati_e = nrow(data_split_e[[1]])
for(i in 2:p_e)
  numero_dati_e = c(numero_dati_e, nrow(data_split_e[[i]]))
plot(numero_dati_e,pch=19,xlab = "Locations", ylab="Number of data in a year", ylim = c(0,370))
abline(h=n_giorni,col = "red", lty = 2, lwd = 3)
```

```{r}
# insert NA where there are hidden missing data
# import info about years
dati_anni = read.delim2("input_data/dati_anni.txt", header=FALSE)
dati_anni = dati_anni[-c(2,5,6)]
anno_vett = c(rep("2014",365),rep("2015",365),rep("2016",366),rep("2017",365),rep("2018",365),rep("2019",365))
dati_anni = cbind(dati_anni,anno_vett)
colnames(dati_anni)=c("numero", "giorno","settimana","anno")
dati_anno = dati_anni[which(dati_anni$anno==anno),]
```

```{r}
# build a matrix 
dati_completi_e = matrix(NA, nrow = n_giorni, ncol = p_e)
colnames(dati_completi_e) = località_e

variabile = "Valore"

for(i in 1:p_e) # scorre le colonne della matrice
{
  k = 1
  n = nrow(data_split_e[[i]])
  for(j in 1:n_giorni)
  {
    if(data_split_e[[i]][k,"Wday"] == dati_anno[j,"giorno"] & k<=n)
    {
      dati_completi_e[j,i] = data_split_e[[i]][k,variabile]
      k=k+1
    }
  }
}
```

```{r}
# view NA's places
vis_miss(data.frame(dati_completi_e))
```

```{r}
# mean by weeks
dati_completi_set_e = matrix(NA, nrow = 52, ncol = p_e)
colnames(dati_completi_set_e) = località_e

for(i in 1:p_e)
{
  for(j in 1:52)
  {
    dati = dati_completi_e[which(dati_anno$settimana == j),i]
    dati_completi_set_e[j,i] = mean(dati,na.rm = TRUE)
    if(is.nan(dati_completi_set_e[j,i])){dati_completi_set_e[j,i]=NA}

  }
}

vis_miss(data.frame(dati_completi_set_e))
```

```{r}
# deal with na: remove columns with more than 10% of NA. If there are few NA impute missing data using interpolation.
source("utils/ts_imputation.R")
index = 0
for(i in 1:p_e)
{
  ts = dati_completi_set_e[,i]
  n_na = sum(is.na(ts))
  if(n_na > 0.05*52){
    index = c(index,i)
  } else if(n_na>0){
    dati_completi_set_e[,i] = ts_imputation(ts)
  }
}
index = index[-1]
indici = 1:p_e
indici = indici[-index]
dati_completi_set_e = dati_completi_set_e[,indici]
p_e = ncol(dati_completi_set_e)
località_e = località_e[indici]
```

```{r}
vis_miss(data.frame(dati_completi_set_e))
```

```{r}
# labels related to locations
area_località_e = rep("",length(indici))
tipo_località_e = rep("",length(indici))
j=1
for (i in indici)
{
  area_località_e[j] = unique(data_split_e[[i]][,"Area"])
  tipo_località_e[j] = unique(data_split_e[[i]][,"Tipo"])
  j=j+1
}
```

## Lombardy

The Lombardy data are already clean, they are only prepared for the mcmc algorithm.

```{r}
source("utils/read_ts_from_lombardia.R")
ob2 =read_ts_from_lombardia()
località_l = ob2$località
area_località_l = ob2$area_località
tipo_località_l = ob2$tipo_località
dati_completi_set_l = ob2$time_series_sett
```


```{r}
# merging data from the two regions
ts_e = dati_completi_set_e
ts_l = dati_completi_set_l
ts = cbind(ts_e, ts_l)
località = c(località_e,località_l)
p = length(località)
area_località = c(area_località_e,area_località_l)
tipo_località = c(tipo_località_e,tipo_località_l)
```


# Data plot

After running this section the plots can be found in `output_plot`



```{r}
source("utils/genera_colori.R")
source("utils/plot_time_series.R")

plot_time_series(data = t(ts_e),
                          label1 = area_località_e,
                          label2 = tipo_località_e,
                          anno = paste(anno,"- Emilia Romagna"))
```

```{r}
plot_time_series(data = t(ts_l),
                           label1 = area_località_l,
                           label2 = tipo_località_l,
                           anno = "2018 - Lombardy")
```

```{r}
plot_time_series(data = t(ts),
                 label1 = area_località,
                 label2 = tipo_località,
                 anno = "2018 - Emilia Romagna and Lombardy")
```

```{r include=FALSE}
plot_e = plot_time_series(data = t(ts_e),
                          label1 = area_località_e,
                          label2 = tipo_località_e,
                          anno = paste(anno,"- Emilia Romagna"))
plot_l = plot_time_series(data = t(ts_l),
                           label1 = area_località_l,
                           label2 = tipo_località_l,
                           anno = "2018 - Lombardy")
plot = plot_time_series(data = t(ts),
                           label1 = area_località,
                           label2 = tipo_località,
                           anno = "2018 - Emilia Romagna and Lombardy")

svg(paste("output_plot/ts_area_emilia_",anno, ".svg",sep=""),width = 9, height = 6)
plot_e$plot1
dev.off()

svg(paste("output_plot/ts_tipo_emilia_",anno, ".svg",sep=""),width = 9, height = 6)
plot_e$plot2
dev.off()

svg(paste("output_plot/ts_area_lombardia_",anno, ".svg",sep=""),width = 9, height = 6)
plot_l$plot1
dev.off()

svg(paste("output_plot/ts_tipo_lombardia_",anno, ".svg",sep=""),width = 9, height = 6)
plot_l$plot2
dev.off()

svg(paste("output_plot/ts_area_unito_",anno, ".svg",sep=""),width = 9, height = 6)
plot$plot1
dev.off()

svg(paste("output_plot/ts_tipo_unito_",anno, ".svg",sep=""),width = 9, height = 6)
plot$plot2
dev.off()

```


# Likelihood choice

```{r}
# remove means from the time series 
medie = colMeans(ts)
ts_no_mean = ts
for (i in 1:p) {
  ts_no_mean[,i] = ts[,i] - medie[i]
}
```

```{r}
# librerie utili
# ts package
# library(itsmr)
library(ggplot2)
library(gridExtra)
# library(forecast)
# library(TSA)
library(tseries)
# library(ggfortify)
```


```{r}
# look at some random Acf
n_plot = 3
plots= vector("list",n_plot*n_plot)
random_indexes = sample(1:p, n_plot*n_plot)
j = 1
for (i in random_indexes) {
  plots[[j]] = ggAcf(ts(ts_no_mean[,i]), lag.max = 52) + labs(title = località[i])
  j = j+1
}
plot_ACF = do.call(grid.arrange, c(plots, ncol = n_plot, nrow = n_plot))
#install.packages("svglite")
ggsave("output_plot/ts_acf_random.svg", plot_ACF, width = 10, height = 10)
```

```{r}
# look at some random Pacf
n_plot = 3
plots= vector("list",n_plot*n_plot)
j = 1
for (i in random_indexes) {
  plots[[j]] = ggPacf(ts(ts_no_mean[,i]), lag.max = 52) + labs(title = località[i])
  j = j+1
}
plot_PACF = do.call(grid.arrange, c(plots, ncol = n_plot, nrow = n_plot))
ggsave("output_plot/ts_pacf_random.svg", plot_PACF, width = 10, height = 10)
```
From the previous plots it seems that an AR1 or AR3 model could be fine

```{r}
fitted_models_AR1 = vector(mode="list",length = p)
fitted_models_best = vector(mode="list",length = p)
coefficienti_AR1 = rep(0,p)
for(i in 1:p)
{
  fitted_models_AR1[[i]] = arima(ts_no_mean[,i], order = c(1,0,0), include.mean = FALSE)
  coefficienti_AR1[i] = fitted_models_AR1[[i]]$coef
  fitted_models_best[[i]] = auto.arima(ts_no_mean[,i])
}

aic_AR1 = rep(0,p)
aic_best = rep(0,p)
for(i in 1:p)
{
  aic_AR1[i] = fitted_models_AR1[[i]]$aic
  aic_best[i] = fitted_models_best[[i]]$aic
}
```

```{r}
plot_aic = plot(aic_AR1, pch=19, xlab = "Locations", ylab="aic",type='l', main="AIC comparison between AR(1) and the best ARIMA model")
points(aic_best, pch=19, col="red",type='l')
legend("topleft", legend = c("AR(1)", "Best ARIMA model"), fill = c("black","red"), bty="n", cex=0.8)
```

```{r}
# save plot
plot(aic_AR1, pch=19, xlab = "Locations", ylab="aic",type='l', main="AIC comparison between AR(1) and the best ARIMA model")
points(aic_best, pch=19, col="red",type='l')
legend("topleft", legend = c("AR(1)", "Best ARIMA model"), fill = c("black","red"), bty="n", cex=0.8)
plot_aic =  recordPlot()
dev.off()
svg(paste("output_plot/aic_comparison_",anno, ".svg",sep=""),width = 9, height = 6)
plot_aic
dev.off()
```


```{r}
source("utils/plot_coefficienti_AR1.R")
plot_coefficienti_AR1(coefficienti_AR1, area_località, tipo_località)
```

```{r}
plot_coeff=plot_coefficienti_AR1(coefficienti_AR1, area_località, tipo_località)

```

```{r}
# save plots
svg(paste("output_plot/coeff_area_emilia_",anno, ".svg",sep=""),width = 9, height = 6)
plot_coeff$plot1
dev.off()

svg(paste("output_plot/coeff_tipo_emilia_",anno, ".svg",sep=""),width = 9, height = 6)
plot_coeff$plot1
dev.off()
```


# Prior density's hyperparameters

Data from Emilia Romagna available in the previous year (2017) is used in order to set the hyperparameters with frequentist estimates.

```{r}
anno = "2017"
ob3 = read_ts_from_emilia(anno = anno)
plot_time_series(data = t(ob3$time_series_sett),
                 label1 = ob3$area_località,
                 label2 = ob3$tipo_località,
                 anno = "2017 - Emilia Romagna")
```

```{r}
plot_old = plot_time_series(data = t(ob3$time_series_sett),
                 label1 = ob3$area_località,
                 label2 = ob3$tipo_località,
                 anno = "2017 - Emilia Romagna")
```

```{r}
svg(paste("output_plot/ts_area_emilia_",anno, ".svg",sep=""),width = 9, height = 6)
plot_old$plot1
dev.off()

svg(paste("output_plot/ts_tipo_emilia_",anno, ".svg",sep=""),width = 9, height = 6)
plot_old$plot2
dev.off()
```



```{r}
source("utils/calcola_param_prior.R")
ob4 = calcola_param_prior(ob3$time_series_sett)
ob4$parametri
plot_coefficienti_AR1(ob4$coefficienti, ob3$area_località, ob3$tipo_località)
```




# Cohesion function's hyperparameters

## Choice of M

One need to include spatial information in order to run the mcmc algorithm.


## Choice of a

Study of the coordinate

```{r}
source("utils/calcola_coordinate.R")
ob5 = calcola_coordinate(località)
coord = ob5$coordinate_lat_long
lonlat_coord = ob5$coordinate_long_lat
```


```{r message=FALSE, warning=FALSE, echo=FALSE}
library(MASS)
library(rgl)
library(DepthProc)
library(hexbin)
library(aplpack)
library(robustbase)
library(raster)
```


Study of `a` with Raster library

```{r}
#trasformo e uso libreria raster
mat_raster <- pointDistance(lonlat_coord, lonlat=T)/ 1000
mat_raster <- lower.tri(mat_raster, diag=FALSE)*mat_raster
mat_raster <- sort(as.numeric(mat_raster[!is.na(mat_raster) & mat_raster>0]))
list(
  range = range(mat_raster),
  mean_d = mean(mat_raster),
  quantiles = quantile(mat_raster, c(0.05,0.25,0.5,0.75, 0.8, 0.95)),
  max = max(mat_raster)
)
```

Study of `a` with Haversine formula

```{r}
cost <- pi/180
n <- nrow(coord)
  dist_matrix <- matrix(NA, n, n)
  for (i in 1:(n - 1)) {
    for (j in (i + 1):n) {
      lat1 <- coord[i, 1]*cost
      lon1 <- coord[i, 2]*cost
      lat2 <- coord[j, 1]*cost
      lon2 <- coord[j, 2]*cost
      a <- sin((lat2 - lat1)/2)^2 + cos(lat1) * cos(lat2) * sin((lon2 - lon1)/2)^2
      c <- 2 * atan2(sqrt(a), sqrt(1 - a))
      d <- 6371 * c
      dist_matrix[i, j] <- d
      dist_matrix[j, i] <- d
    }
  }
#dist_matrix
  
  matd3 <- lower.tri(dist_matrix, diag=FALSE)*dist_matrix
  matd3 <- sort(as.numeric(matd3[!is.na(matd3) & matd3>0]))
list(
  range = range(matd3),
  mean_d = mean(matd3),
  quantiles = quantile(matd3, c(0.05,0.25,0.5,0.75, 0.8, 0.95)),
  max = max(matd3)
)
```

# Output saving

```{r}
# Salvo i dati
folder = "bayesmix/resources/datasets/"

write.table(t(ts_no_mean),sep=",", row.names = FALSE, col.names=FALSE , file = paste(folder, "ts.csv", sep=""))

write.table(t(ts),sep=",", row.names = FALSE, col.names=FALSE , file = paste(folder, "ts_mean.csv", sep=""))

write.table(coord,sep=",", row.names = FALSE, col.names=FALSE , file = paste(folder, "coord.csv", sep=""))
```








# MCMC algorithm

Now everything is ready to run the mcmc algorithm.

To set the desired parameter open an Ubuntu terminal and run the following in the `PM10_BAYESIAN` folder:

```{r}
# # Number of iterations and burnin
# nano bayesmix/examples/ar1nig_hierarchy/in/algo.asciipb
# 
# # Cohesion function's hyperparameters
# nano bayesmix/examples/ar1nig_hierarchy/in/dp.asciipb
# 
# # Prior's hyperparameters (don't change unless you have a good reason)
# nano bayesmix/examples/ar1nig_hierarchy/in/ts.asciipb
```


To run the mcmc algo:

```{r}
# cd bayesmix
# 
# nano bayesmix/examples/ar1nig_hierarchy/in/algo.asciipb
# 
# # Cohesion function's hyperparameters
# nano bayesmix/examples/ar1nig_hierarchy/in/dp.asciipb
# 
# # Prior's hyperparameters (don't change unless you have a good reason)
# nano bayesmix/examples/ar1nig_hierarchy/in/ts.asciipb
```


# Results

## Number of clusters's distribution

```{r}

```

## Best partition

```{r}
num_clust = read.csv("bayesmix/examples/ar1nig_hierarchy/out/numclust.csv", header = FALSE)
num_clust = num_clust$V1
hist(num_clust, prob=F, main="", xlab="Number of clusters", xlim= c(0, max(num_clust)))

partitions = read.csv("bayesmix/examples/ar1nig_hierarchy/out/clustering.csv", header = FALSE)

state_params = read.csv("bayesmix/examples/ar1nig_hierarchy/out/stateparams.csv", header = FALSE)

coord <- read.csv("bayesmix/resources/datasets/coord.csv", header=F)

colnames(coord) <- c("lat","lon")

ts <- read.csv("bayesmix/resources/datasets/ts_mean.csv", header=F)


source("utils/plot_clustering.R")
cohesion_params = read.table('used_parameters.txt', header=F)
M <- cohesion_params[1,1]
a <- cohesion_params[2,1]

# in this way it prints any output
plot_clustering(partitions, coord, "VI" ,M, a, ts, state_params)
ob6 = plot_clustering(partitions, coord, "VI" ,M, a, ts, state_params)

nome = paste("map_M_", M, "_a_",a,sep="")
folder = "output_plot/"

save_svg_plot(ob6$plot,name=nome, folder=folder, type="ggplot",width = 10, height = 10)
# in this way it suppress output
#b<-plot_clustering(data2, coord, "VI" ,M, a, ts[,-1], clus_chain)

# to plot it write (in the console):
# x11();ob6$plot
```

```{r}
plot_map = 
```


```{r}
ob6$posterior_unique_vals
```

```{r}
plot(1:5,1:5)
points(2,3)
plot1= recordPlot()
```

```{r}
source("utils/save_svg_plot.R")
save_svg_plot(plot1)
```
```{r}
replayPlot(plot1)

```

